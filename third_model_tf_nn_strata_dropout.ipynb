{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "k4i7UDH919kL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RYv8pJ0nA7-v"
   },
   "outputs": [],
   "source": [
    "pd_train = pd.read_csv(\"Dataset/arxiv/train_all.csv\",dtype=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HglJaUiUi708"
   },
   "outputs": [],
   "source": [
    "pd_test = pd.read_csv(\"Dataset/arxiv/test_all.csv\",dtype=\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jSekhlzYnRuA",
    "outputId": "1e69cd5c-0664-40f8-b651-b9d0f0c427c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200092, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UNzJd7A2FPeM",
    "outputId": "57ef1791-0795-4c13-947f-7d191ddb5b29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37125, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCgx4jXHj-6r"
   },
   "source": [
    "Clean up abstract using noun only phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "KTpdT-x1lyTp",
    "outputId": "b61b3705-24a3-4d52-bdf0-ba3009b82b16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gRTj9pzItAIP"
   },
   "outputs": [],
   "source": [
    "def noun_only_sentence(x_string):\n",
    "    string_list = x_string.lower().split(\".\")\n",
    "    noun_sentence = []\n",
    "    for my_str in string_list:\n",
    "        #print(my_str)\n",
    "        tokens = word_tokenize(my_str)\n",
    "        #bigrams = ngrams(token,2)\n",
    "        # remove all tokens that are not alphabetic\n",
    "        is_noun = lambda pos: pos[:2] == 'NN'\n",
    "        # do the nlp stuff\n",
    "        nouns = [word for (word, pos) in nltk.pos_tag(tokens) if is_noun(pos)] \n",
    "        noun_sentence.append(nouns)\n",
    "    return noun_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XR0Ue2lKldH5"
   },
   "outputs": [],
   "source": [
    "#abstract_noun = pd_train.abstract.apply(lambda x: noun_only_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9mFnz_bb8Vyb"
   },
   "outputs": [],
   "source": [
    "#abstract_noun_test = pd_test.abstract.apply(lambda x: noun_only_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Q49nh0fkmAPI"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Dataset/arxiv/abstract_noun_strat.pickle\",\"rb\") as file:\n",
    "  (abstract_noun,abstract_noun_test) = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gZt8ozMmqnYZ"
   },
   "outputs": [],
   "source": [
    "# clean abstract noun so it will contain only alphabetic char\n",
    "\n",
    "# use english stop_words\n",
    "stop_words =  list(stopwords.words('english'))\n",
    "\n",
    "def alphabetic_only_l(x_string):\n",
    "  list_sent = []    \n",
    "  for tokens in x_string:\n",
    "    # remove all tokens that are not alphabetic\n",
    "    words = [word for word in tokens if word.isalpha() and word not in stop_words and len(word)>3]\n",
    "    list_sent.append(words)    \n",
    "  return list_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZzR6AvzFrUMY"
   },
   "outputs": [],
   "source": [
    "#abs_noun_clean = abstract_noun.apply(lambda x:alphabetic_only_l(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AaYWBsJl9o4c"
   },
   "outputs": [],
   "source": [
    "#abs_noun_clean_test = abstract_noun_test.apply(lambda x:alphabetic_only_l(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oWpiT7DtrXP5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Dataset/arxiv/abstract_noun_clean_strat.pickle\",\"rb\") as file:\n",
    "  (abs_noun_clean,abs_noun_clean_test) = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ejix081jsZFS"
   },
   "outputs": [],
   "source": [
    "# features/words chooser using tfidf\n",
    "def sent_convert(x_string):\n",
    "  list_sent = []\n",
    "  for sent in x_string:\n",
    "    # remove all tokens that are not alphabetic    \n",
    "    list_sent.append(\" \".join(sent))    \n",
    "  return \". \".join(list_sent)\n",
    "\n",
    "abs_noun_sent = abs_noun_clean.apply(lambda x:sent_convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rLxvhHs09woQ"
   },
   "outputs": [],
   "source": [
    "abs_noun_sent_test = abs_noun_clean_test.apply(lambda x:sent_convert(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IAERrdyUm1JG",
    "outputId": "7132e773-4ae8-486b-f593-d95feb48ca13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200092,)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_noun_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "agq99pErsuta"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2)\n",
    "tf = tf_vectorizer.fit_transform(abs_noun_sent.values)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(abs_noun_sent.values)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KUvRysAPtDwD",
    "outputId": "f450c63a-29eb-4324-a125-957ae43d5a1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44819"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "mDHfKZDYMtng",
    "outputId": "ff4a78ce-d64b-4bd1-a9de-ae9f085c31a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/ec/e9ce1b52e71f6dff3bd944f020cef7140779e783ab27512ea7c7275ddee5/tflearn-0.3.2.tar.gz (98kB)\n",
      "\r",
      "\u001b[K    10% |███▎                            | 10kB 22.2MB/s eta 0:00:01\r",
      "\u001b[K    20% |██████▋                         | 20kB 4.3MB/s eta 0:00:01\r",
      "\u001b[K    31% |██████████                      | 30kB 6.1MB/s eta 0:00:01\r",
      "\u001b[K    41% |█████████████▎                  | 40kB 4.1MB/s eta 0:00:01\r",
      "\u001b[K    51% |████████████████▋               | 51kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K    62% |████████████████████            | 61kB 5.8MB/s eta 0:00:01\r",
      "\u001b[K    72% |███████████████████████▎        | 71kB 6.5MB/s eta 0:00:01\r",
      "\u001b[K    83% |██████████████████████████▋     | 81kB 7.2MB/s eta 0:00:01\r",
      "\u001b[K    93% |██████████████████████████████  | 92kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K    100% |████████████████████████████████| 102kB 5.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.14.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.11.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (4.0.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn) (0.46)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Running setup.py bdist_wheel for tflearn ... \u001b[?25l-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d0/f6/69/0ef3ee395aac2e5d15d89efd29a9a216f3c27767b43b72c006\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HvnICvaZKYWt",
    "outputId": "9ecd3640-389c-40eb-dc85-0888cdc2113b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HmcdMl4kQLIc"
   },
   "outputs": [],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Ugw1veqHRDGZ"
   },
   "outputs": [],
   "source": [
    "y_train_label = pd_train.cats.apply(lambda x: x.split(\"|\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1Hcziddq9W7U",
    "outputId": "454d50a5-f602-4e99-eff2-54b4614218a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200092, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xJLQukrCRayi"
   },
   "outputs": [],
   "source": [
    "y_train_label_l = list(y_train_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5d4nizj7RBuK"
   },
   "outputs": [],
   "source": [
    "y_train_class = y_train_label.apply(lambda x:y_train_label_l.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "znXBVKuLMzjl"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "# input layer\n",
    "model.add(Dropout(0.4,input_shape=(tfidf.shape[1],)))\n",
    "model.add(keras.layers.Dense(5000,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# hidden layer\n",
    "model.add(keras.layers.Dense(1000,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(keras.layers.Dense(500,activation='relu'))\n",
    "model.add(keras.layers.Dense(100,activation='relu'))\n",
    "\n",
    "# output layer\n",
    "model.add(keras.layers.Dense(len(y_train_label_l),activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "UbwgVzLNduGj",
    "outputId": "ae85de51-0f3e-49a6-d50c-a908d47ad572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_12 (Dropout)         (None, 44819)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5000)              224100000 \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1000)              5001000   \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 229,653,620\n",
      "Trainable params: 229,653,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fUfLbtA0XAJm"
   },
   "outputs": [],
   "source": [
    "y_test_class = pd_test.cats.apply(lambda x: x.split(\"|\")[0]).apply(lambda x:y_train_label_l.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XbRyRujGXO7L"
   },
   "outputs": [],
   "source": [
    "tfidf_test = tfidf_vectorizer.transform(abs_noun_sent_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QnOi90KLYKge"
   },
   "outputs": [],
   "source": [
    "y_test_one_hot = to_categorical(y_test_class.values,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2550
    },
    "colab_type": "code",
    "id": "NGMK2ht7T9-F",
    "outputId": "e5e51a0e-1db7-4296-bebc-26046d7312b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 45s 2ms/step - loss: 1.5103 - acc: 0.5546\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.9797 - acc: 0.7083\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.8677 - acc: 0.7324\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.8341 - acc: 0.7441\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.7880 - acc: 0.7548\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.7829 - acc: 0.7573\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.7527 - acc: 0.7644\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.7408 - acc: 0.7644\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.7350 - acc: 0.7692\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.7277 - acc: 0.7728\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.7097 - acc: 0.7731\n",
      "accuracy: 0.8036094276094277\n",
      "kappa: 0.7684719741477273\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.85      0.88      9054\n",
      "          1       0.60      0.67      0.63      3187\n",
      "          2       0.93      0.93      0.93      3980\n",
      "          3       0.68      0.64      0.66       798\n",
      "          4       0.74      0.66      0.70      1065\n",
      "          5       0.69      0.57      0.63       379\n",
      "          6       0.66      0.46      0.54      1204\n",
      "          7       0.81      0.83      0.82      4324\n",
      "          8       0.79      0.94      0.86      8125\n",
      "          9       0.75      0.87      0.80      1354\n",
      "         10       0.48      0.33      0.39       132\n",
      "         11       0.75      0.74      0.74       214\n",
      "         12       0.81      0.68      0.74      1498\n",
      "         13       0.46      0.17      0.25       442\n",
      "         14       0.60      0.26      0.36       261\n",
      "         15       0.62      0.54      0.58       504\n",
      "         16       0.80      0.59      0.68       263\n",
      "         17       0.84      0.57      0.68       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.00      0.00      0.00       129\n",
      "\n",
      "avg / total       0.80      0.80      0.80     37125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.6776 - acc: 0.7851\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.6438 - acc: 0.7904\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.6202 - acc: 0.7988\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.6215 - acc: 0.7994\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.6014 - acc: 0.8035\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.6028 - acc: 0.8045\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.5882 - acc: 0.8053\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.5848 - acc: 0.8074\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.5896 - acc: 0.8050\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.5765 - acc: 0.8122\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.5767 - acc: 0.8080\n",
      "accuracy: 0.818047138047138\n",
      "kappa: 0.7863781048191179\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.89      0.89      9054\n",
      "          1       0.67      0.63      0.65      3187\n",
      "          2       0.94      0.92      0.93      3980\n",
      "          3       0.65      0.78      0.71       798\n",
      "          4       0.77      0.68      0.72      1065\n",
      "          5       0.76      0.58      0.66       379\n",
      "          6       0.65      0.52      0.58      1204\n",
      "          7       0.82      0.86      0.84      4324\n",
      "          8       0.85      0.90      0.87      8125\n",
      "          9       0.80      0.84      0.82      1354\n",
      "         10       0.52      0.61      0.56       132\n",
      "         11       0.68      0.75      0.71       214\n",
      "         12       0.68      0.81      0.74      1498\n",
      "         13       0.45      0.22      0.30       442\n",
      "         14       0.53      0.40      0.46       261\n",
      "         15       0.64      0.61      0.63       504\n",
      "         16       0.70      0.70      0.70       263\n",
      "         17       0.77      0.77      0.77       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.00      0.00      0.00       129\n",
      "\n",
      "avg / total       0.81      0.82      0.81     37125\n",
      "\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.5595 - acc: 0.8136\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.5363 - acc: 0.8231\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.5296 - acc: 0.8273\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.5210 - acc: 0.8257\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.5093 - acc: 0.8310\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.5188 - acc: 0.8277\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.4906 - acc: 0.8387\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.4980 - acc: 0.8341\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.4986 - acc: 0.8331\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.4855 - acc: 0.8381\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.4878 - acc: 0.8376\n",
      "accuracy: 0.8193939393939393\n",
      "kappa: 0.7876556981269818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.91      0.89      9054\n",
      "          1       0.71      0.58      0.64      3187\n",
      "          2       0.91      0.95      0.93      3980\n",
      "          3       0.69      0.69      0.69       798\n",
      "          4       0.74      0.72      0.73      1065\n",
      "          5       0.76      0.60      0.67       379\n",
      "          6       0.61      0.54      0.57      1204\n",
      "          7       0.80      0.90      0.84      4324\n",
      "          8       0.86      0.88      0.87      8125\n",
      "          9       0.78      0.88      0.82      1354\n",
      "         10       0.52      0.55      0.53       132\n",
      "         11       0.71      0.73      0.72       214\n",
      "         12       0.79      0.72      0.75      1498\n",
      "         13       0.50      0.23      0.32       442\n",
      "         14       0.47      0.53      0.50       261\n",
      "         15       0.64      0.59      0.61       504\n",
      "         16       0.74      0.68      0.71       263\n",
      "         17       0.89      0.70      0.78       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.00      0.00      0.00       129\n",
      "\n",
      "avg / total       0.81      0.82      0.81     37125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "mini_batch = 20000\n",
    "total_docs = tfidf.shape[0]\n",
    "total_batch = int(tfidf.shape[0]/mini_batch)+1\n",
    "#range_docs = range(0,total_docs)\n",
    "#range_docs = train\n",
    "\n",
    "\n",
    "for it in range(3):\n",
    "  k_sample = StratifiedKFold(n_splits=total_batch, shuffle=True, random_state=1).split(range(len(y_train_class.values)), y_train_class.values)\n",
    "  for batch in k_sample:\n",
    "      # random sample dataset\n",
    "      #r_index = random.sample(list(range_docs),mini_batch)\n",
    "      #r_index_start = mini_batch*batch\n",
    "      #r_index_end = mini_batch*(batch+1) if batch<total_batch-1 else total_docs\n",
    "      X_batch = tfidf[batch[1]]\n",
    "      #y_batch = dataset.iloc[r_index].subjects.apply(lambda x:labeling_subjects(x))\n",
    "      #y_batch = np.array([])\n",
    "      #for x in dataset.iloc[r_index].subjects:\n",
    "      #    y_batch = np.append(y_batch,labeling_subjects(x),axis=0)\n",
    "      #y_batch\n",
    "      #y_batch = y_batch.reshape(len(r_index),len(label_class))\n",
    "      y_batch = to_categorical(y_train_class[batch[1]].values,20)\n",
    "\n",
    "      model.fit(X_batch,y_batch,epochs=1,batch_size=200)\n",
    "  y_pred = model.predict_classes(tfidf_test)\n",
    "  from sklearn.metrics import classification_report,accuracy_score,cohen_kappa_score\n",
    "  acc = accuracy_score(y_test_class,y_pred)\n",
    "  kap = cohen_kappa_score(y_test_class,y_pred)\n",
    "  print(\"accuracy:\",acc)\n",
    "  print(\"kappa:\",kap)\n",
    "  print(classification_report(y_test_class,y_pred))\n",
    "  model.save(\"Dataset/arxiv/nn-model-drop-{}.h5\".format(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4216
    },
    "colab_type": "code",
    "id": "lE9HBP_6kgQU",
    "outputId": "9c0a61c7-0ced-4256-d479-1f87df678803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.4725 - acc: 0.8408\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.4559 - acc: 0.8486\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.4464 - acc: 0.8521\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.4330 - acc: 0.8520\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.4357 - acc: 0.8503\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.4365 - acc: 0.8506\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.4198 - acc: 0.8583\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.4209 - acc: 0.8578\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.4144 - acc: 0.8595\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.4086 - acc: 0.8608\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.3998 - acc: 0.8633\n",
      "accuracy: 0.8224646464646465\n",
      "kappa: 0.791094397538806\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.88      0.89      9054\n",
      "          1       0.68      0.63      0.66      3187\n",
      "          2       0.92      0.95      0.93      3980\n",
      "          3       0.68      0.72      0.70       798\n",
      "          4       0.75      0.71      0.73      1065\n",
      "          5       0.76      0.55      0.64       379\n",
      "          6       0.67      0.48      0.56      1204\n",
      "          7       0.83      0.88      0.85      4324\n",
      "          8       0.83      0.92      0.87      8125\n",
      "          9       0.82      0.84      0.83      1354\n",
      "         10       0.45      0.64      0.53       132\n",
      "         11       0.76      0.74      0.75       214\n",
      "         12       0.74      0.78      0.76      1498\n",
      "         13       0.48      0.16      0.24       442\n",
      "         14       0.60      0.42      0.49       261\n",
      "         15       0.61      0.59      0.60       504\n",
      "         16       0.70      0.72      0.71       263\n",
      "         17       0.92      0.70      0.80       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.00      0.00      0.00       129\n",
      "\n",
      "avg / total       0.81      0.82      0.82     37125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.3902 - acc: 0.8701\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.3767 - acc: 0.8730\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.3711 - acc: 0.8754\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.3694 - acc: 0.8757\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.3563 - acc: 0.8791\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.3564 - acc: 0.8823\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.3400 - acc: 0.8842\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.3454 - acc: 0.8846\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.3457 - acc: 0.8839\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.3403 - acc: 0.8877\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.3350 - acc: 0.8866\n",
      "accuracy: 0.8204983164983165\n",
      "kappa: 0.7890800963204048\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.90      0.89      9054\n",
      "          1       0.74      0.56      0.64      3187\n",
      "          2       0.91      0.95      0.93      3980\n",
      "          3       0.72      0.66      0.69       798\n",
      "          4       0.73      0.74      0.74      1065\n",
      "          5       0.68      0.71      0.69       379\n",
      "          6       0.59      0.60      0.59      1204\n",
      "          7       0.78      0.91      0.84      4324\n",
      "          8       0.85      0.89      0.87      8125\n",
      "          9       0.82      0.84      0.83      1354\n",
      "         10       0.54      0.45      0.49       132\n",
      "         11       0.81      0.68      0.74       214\n",
      "         12       0.79      0.70      0.74      1498\n",
      "         13       0.49      0.20      0.28       442\n",
      "         14       0.52      0.48      0.50       261\n",
      "         15       0.56      0.69      0.62       504\n",
      "         16       0.69      0.71      0.70       263\n",
      "         17       0.82      0.82      0.82       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.00      0.00      0.00       129\n",
      "\n",
      "avg / total       0.81      0.82      0.81     37125\n",
      "\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.3302 - acc: 0.8900\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.3211 - acc: 0.8945\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.3130 - acc: 0.8961\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.3035 - acc: 0.8988\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.3074 - acc: 0.8948\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.2959 - acc: 0.8981\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.2893 - acc: 0.9017\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.2904 - acc: 0.9034\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.2889 - acc: 0.9021\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.2880 - acc: 0.9012\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.2858 - acc: 0.9014\n",
      "accuracy: 0.8216026936026936\n",
      "kappa: 0.7905926228294862\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.89      0.89      9054\n",
      "          1       0.68      0.63      0.65      3187\n",
      "          2       0.92      0.95      0.93      3980\n",
      "          3       0.72      0.63      0.68       798\n",
      "          4       0.66      0.80      0.72      1065\n",
      "          5       0.64      0.72      0.68       379\n",
      "          6       0.63      0.54      0.59      1204\n",
      "          7       0.81      0.88      0.85      4324\n",
      "          8       0.86      0.89      0.88      8125\n",
      "          9       0.82      0.86      0.84      1354\n",
      "         10       0.61      0.36      0.45       132\n",
      "         11       0.61      0.81      0.70       214\n",
      "         12       0.77      0.73      0.75      1498\n",
      "         13       0.52      0.17      0.26       442\n",
      "         14       0.49      0.49      0.49       261\n",
      "         15       0.59      0.63      0.61       504\n",
      "         16       0.83      0.65      0.73       263\n",
      "         17       0.82      0.80      0.81       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.00      0.00      0.00       129\n",
      "\n",
      "avg / total       0.81      0.82      0.82     37125\n",
      "\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.2762 - acc: 0.9046\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2715 - acc: 0.9093\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2705 - acc: 0.9084\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2567 - acc: 0.9134\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.2581 - acc: 0.9126\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.2534 - acc: 0.9131\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.2572 - acc: 0.9139\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.2549 - acc: 0.9119\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.2526 - acc: 0.9157\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.2537 - acc: 0.9138\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.2454 - acc: 0.9181\n",
      "accuracy: 0.8211986531986532\n",
      "kappa: 0.7897300627035357\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.88      0.89      9054\n",
      "          1       0.71      0.58      0.64      3187\n",
      "          2       0.94      0.94      0.94      3980\n",
      "          3       0.71      0.68      0.70       798\n",
      "          4       0.73      0.74      0.73      1065\n",
      "          5       0.65      0.75      0.69       379\n",
      "          6       0.60      0.57      0.58      1204\n",
      "          7       0.79      0.90      0.84      4324\n",
      "          8       0.83      0.91      0.87      8125\n",
      "          9       0.85      0.83      0.84      1354\n",
      "         10       0.50      0.50      0.50       132\n",
      "         11       0.76      0.75      0.75       214\n",
      "         12       0.76      0.75      0.75      1498\n",
      "         13       0.50      0.22      0.30       442\n",
      "         14       0.55      0.45      0.50       261\n",
      "         15       0.63      0.59      0.61       504\n",
      "         16       0.75      0.68      0.71       263\n",
      "         17       0.96      0.67      0.79       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.50      0.01      0.02       129\n",
      "\n",
      "avg / total       0.82      0.82      0.82     37125\n",
      "\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.2453 - acc: 0.9174\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2261 - acc: 0.9244\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2288 - acc: 0.9211\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2327 - acc: 0.9216\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.2262 - acc: 0.9228\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.2257 - acc: 0.9256\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.2163 - acc: 0.9270\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.2203 - acc: 0.9262\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.2246 - acc: 0.9263\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.2205 - acc: 0.9287\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.2168 - acc: 0.9276\n",
      "accuracy: 0.8245387205387206\n",
      "kappa: 0.7942055603894395\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.89      0.89      9054\n",
      "          1       0.68      0.64      0.66      3187\n",
      "          2       0.95      0.93      0.94      3980\n",
      "          3       0.69      0.70      0.69       798\n",
      "          4       0.73      0.76      0.74      1065\n",
      "          5       0.76      0.59      0.66       379\n",
      "          6       0.63      0.56      0.59      1204\n",
      "          7       0.81      0.89      0.85      4324\n",
      "          8       0.86      0.89      0.87      8125\n",
      "          9       0.80      0.89      0.84      1354\n",
      "         10       0.51      0.61      0.56       132\n",
      "         11       0.76      0.73      0.74       214\n",
      "         12       0.74      0.76      0.75      1498\n",
      "         13       0.49      0.23      0.31       442\n",
      "         14       0.55      0.48      0.51       261\n",
      "         15       0.57      0.68      0.62       504\n",
      "         16       0.77      0.69      0.73       263\n",
      "         17       0.93      0.76      0.83       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.40      0.02      0.03       129\n",
      "\n",
      "avg / total       0.82      0.82      0.82     37125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for it in range(5):\n",
    "  k_sample = StratifiedKFold(n_splits=total_batch, shuffle=True, random_state=1).split(range(len(y_train_class.values)), y_train_class.values)\n",
    "  for batch in k_sample:\n",
    "      # random sample dataset\n",
    "      #r_index = random.sample(list(range_docs),mini_batch)\n",
    "      #r_index_start = mini_batch*batch\n",
    "      #r_index_end = mini_batch*(batch+1) if batch<total_batch-1 else total_docs\n",
    "      X_batch = tfidf[batch[1]]\n",
    "      #y_batch = dataset.iloc[r_index].subjects.apply(lambda x:labeling_subjects(x))\n",
    "      #y_batch = np.array([])\n",
    "      #for x in dataset.iloc[r_index].subjects:\n",
    "      #    y_batch = np.append(y_batch,labeling_subjects(x),axis=0)\n",
    "      #y_batch\n",
    "      #y_batch = y_batch.reshape(len(r_index),len(label_class))\n",
    "      y_batch = to_categorical(y_train_class[batch[1]].values,20)\n",
    "\n",
    "      model.fit(X_batch,y_batch,epochs=1,batch_size=200)\n",
    "  y_pred = model.predict_classes(tfidf_test)\n",
    "  from sklearn.metrics import classification_report,accuracy_score,cohen_kappa_score\n",
    "  acc = accuracy_score(y_test_class,y_pred)\n",
    "  kap = cohen_kappa_score(y_test_class,y_pred)\n",
    "  print(\"accuracy:\",acc)\n",
    "  print(\"kappa:\",kap)\n",
    "  print(classification_report(y_test_class,y_pred))\n",
    "  model.save(\"Dataset/arxiv/nn-model-drop-b{}.h5\".format(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4216
    },
    "colab_type": "code",
    "id": "EI3F6Fdnt5eE",
    "outputId": "dd8527a4-9bae-44ea-88cc-31b7d42bba1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2207 - acc: 0.9260\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1968 - acc: 0.9345\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2114 - acc: 0.9305\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.2155 - acc: 0.9300\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.2033 - acc: 0.9321\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.2026 - acc: 0.9319\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.2027 - acc: 0.9330\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1995 - acc: 0.9338\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.1991 - acc: 0.9330\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.1979 - acc: 0.9340\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.1996 - acc: 0.9351\n",
      "accuracy: 0.8222760942760943\n",
      "kappa: 0.7913128626841631\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.88      0.89      9054\n",
      "          1       0.69      0.63      0.66      3187\n",
      "          2       0.93      0.94      0.94      3980\n",
      "          3       0.66      0.76      0.70       798\n",
      "          4       0.77      0.69      0.73      1065\n",
      "          5       0.67      0.69      0.68       379\n",
      "          6       0.62      0.55      0.58      1204\n",
      "          7       0.81      0.89      0.84      4324\n",
      "          8       0.84      0.91      0.87      8125\n",
      "          9       0.84      0.84      0.84      1354\n",
      "         10       0.48      0.62      0.54       132\n",
      "         11       0.77      0.75      0.76       214\n",
      "         12       0.80      0.70      0.75      1498\n",
      "         13       0.46      0.29      0.36       442\n",
      "         14       0.49      0.49      0.49       261\n",
      "         15       0.60      0.66      0.63       504\n",
      "         16       0.77      0.69      0.73       263\n",
      "         17       0.92      0.76      0.83       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       1.00      0.02      0.03       129\n",
      "\n",
      "avg / total       0.82      0.82      0.82     37125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.1944 - acc: 0.9370\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1970 - acc: 0.9347\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1979 - acc: 0.9336\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1872 - acc: 0.9389\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.1957 - acc: 0.9343\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.1892 - acc: 0.9380\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1825 - acc: 0.9413\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1870 - acc: 0.9373\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.1864 - acc: 0.9366\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.1874 - acc: 0.9388\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.1881 - acc: 0.9378\n",
      "accuracy: 0.822949494949495\n",
      "kappa: 0.7918135007901929\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.88      0.89      9054\n",
      "          1       0.67      0.67      0.67      3187\n",
      "          2       0.93      0.94      0.94      3980\n",
      "          3       0.68      0.72      0.70       798\n",
      "          4       0.76      0.70      0.73      1065\n",
      "          5       0.73      0.62      0.67       379\n",
      "          6       0.66      0.48      0.55      1204\n",
      "          7       0.83      0.87      0.85      4324\n",
      "          8       0.84      0.91      0.87      8125\n",
      "          9       0.87      0.81      0.84      1354\n",
      "         10       0.50      0.47      0.48       132\n",
      "         11       0.72      0.78      0.75       214\n",
      "         12       0.73      0.78      0.76      1498\n",
      "         13       0.47      0.24      0.32       442\n",
      "         14       0.58      0.41      0.48       261\n",
      "         15       0.62      0.59      0.60       504\n",
      "         16       0.75      0.69      0.72       263\n",
      "         17       0.78      0.85      0.81       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       1.00      0.02      0.03       129\n",
      "\n",
      "avg / total       0.82      0.82      0.82     37125\n",
      "\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.1806 - acc: 0.9414\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1738 - acc: 0.9439\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1843 - acc: 0.9389\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1667 - acc: 0.9447\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.1708 - acc: 0.9448\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.1668 - acc: 0.9446\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1673 - acc: 0.9462\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1707 - acc: 0.9437\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.1841 - acc: 0.9408\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.1709 - acc: 0.9434\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.1785 - acc: 0.9417\n",
      "accuracy: 0.8235690235690236\n",
      "kappa: 0.7926677080263156\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.89      0.89      9054\n",
      "          1       0.68      0.64      0.66      3187\n",
      "          2       0.94      0.94      0.94      3980\n",
      "          3       0.70      0.69      0.69       798\n",
      "          4       0.74      0.74      0.74      1065\n",
      "          5       0.71      0.66      0.69       379\n",
      "          6       0.63      0.53      0.57      1204\n",
      "          7       0.80      0.90      0.85      4324\n",
      "          8       0.85      0.90      0.88      8125\n",
      "          9       0.83      0.87      0.85      1354\n",
      "         10       0.46      0.54      0.49       132\n",
      "         11       0.75      0.74      0.74       214\n",
      "         12       0.80      0.71      0.75      1498\n",
      "         13       0.45      0.27      0.34       442\n",
      "         14       0.50      0.49      0.50       261\n",
      "         15       0.62      0.57      0.59       504\n",
      "         16       0.82      0.65      0.72       263\n",
      "         17       0.84      0.75      0.80       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       1.00      0.01      0.02       129\n",
      "\n",
      "avg / total       0.82      0.82      0.82     37125\n",
      "\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.1677 - acc: 0.9448\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.1662 - acc: 0.9453\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1688 - acc: 0.9443\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1654 - acc: 0.9466\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.1679 - acc: 0.9451\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.1699 - acc: 0.9446\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1644 - acc: 0.9462\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1571 - acc: 0.9474\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.1564 - acc: 0.9483\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.1631 - acc: 0.9461\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.1683 - acc: 0.9464\n",
      "accuracy: 0.8233265993265994\n",
      "kappa: 0.7925751278990513\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.89      0.89      9054\n",
      "          1       0.72      0.58      0.64      3187\n",
      "          2       0.92      0.95      0.94      3980\n",
      "          3       0.67      0.71      0.69       798\n",
      "          4       0.75      0.72      0.73      1065\n",
      "          5       0.72      0.68      0.70       379\n",
      "          6       0.60      0.56      0.58      1204\n",
      "          7       0.80      0.90      0.85      4324\n",
      "          8       0.85      0.90      0.87      8125\n",
      "          9       0.82      0.88      0.85      1354\n",
      "         10       0.55      0.52      0.53       132\n",
      "         11       0.72      0.77      0.74       214\n",
      "         12       0.77      0.73      0.75      1498\n",
      "         13       0.43      0.27      0.33       442\n",
      "         14       0.59      0.46      0.52       261\n",
      "         15       0.55      0.67      0.61       504\n",
      "         16       0.80      0.67      0.73       263\n",
      "         17       0.90      0.72      0.80       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       1.00      0.02      0.03       129\n",
      "\n",
      "avg / total       0.82      0.82      0.82     37125\n",
      "\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.1623 - acc: 0.9451\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 36s 2ms/step - loss: 0.1545 - acc: 0.9492\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1538 - acc: 0.9470\n",
      "Epoch 1/1\n",
      "18195/18195 [==============================] - 35s 2ms/step - loss: 0.1579 - acc: 0.9470\n",
      "Epoch 1/1\n",
      "18193/18193 [==============================] - 35s 2ms/step - loss: 0.1567 - acc: 0.9465\n",
      "Epoch 1/1\n",
      "18191/18191 [==============================] - 35s 2ms/step - loss: 0.1583 - acc: 0.9484\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1558 - acc: 0.9494\n",
      "Epoch 1/1\n",
      "18190/18190 [==============================] - 35s 2ms/step - loss: 0.1560 - acc: 0.9498\n",
      "Epoch 1/1\n",
      "18188/18188 [==============================] - 35s 2ms/step - loss: 0.1543 - acc: 0.9497\n",
      "Epoch 1/1\n",
      "18182/18182 [==============================] - 35s 2ms/step - loss: 0.1576 - acc: 0.9473\n",
      "Epoch 1/1\n",
      "18178/18178 [==============================] - 35s 2ms/step - loss: 0.1480 - acc: 0.9529\n",
      "accuracy: 0.8241077441077441\n",
      "kappa: 0.7932320593261615\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.89      0.89      9054\n",
      "          1       0.71      0.62      0.66      3187\n",
      "          2       0.93      0.95      0.94      3980\n",
      "          3       0.73      0.63      0.68       798\n",
      "          4       0.75      0.73      0.74      1065\n",
      "          5       0.76      0.63      0.69       379\n",
      "          6       0.66      0.50      0.57      1204\n",
      "          7       0.82      0.88      0.85      4324\n",
      "          8       0.84      0.91      0.87      8125\n",
      "          9       0.81      0.88      0.85      1354\n",
      "         10       0.56      0.54      0.55       132\n",
      "         11       0.67      0.81      0.73       214\n",
      "         12       0.71      0.78      0.74      1498\n",
      "         13       0.43      0.26      0.32       442\n",
      "         14       0.59      0.43      0.50       261\n",
      "         15       0.60      0.62      0.61       504\n",
      "         16       0.81      0.67      0.73       263\n",
      "         17       0.90      0.72      0.80       195\n",
      "         18       0.00      0.00      0.00        17\n",
      "         19       0.50      0.02      0.03       129\n",
      "\n",
      "avg / total       0.82      0.82      0.82     37125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for it in range(5):\n",
    "  k_sample = StratifiedKFold(n_splits=total_batch, shuffle=True, random_state=1).split(range(len(y_train_class.values)), y_train_class.values)\n",
    "  for batch in k_sample:\n",
    "      # random sample dataset\n",
    "      #r_index = random.sample(list(range_docs),mini_batch)\n",
    "      #r_index_start = mini_batch*batch\n",
    "      #r_index_end = mini_batch*(batch+1) if batch<total_batch-1 else total_docs\n",
    "      X_batch = tfidf[batch[1]]\n",
    "      #y_batch = dataset.iloc[r_index].subjects.apply(lambda x:labeling_subjects(x))\n",
    "      #y_batch = np.array([])\n",
    "      #for x in dataset.iloc[r_index].subjects:\n",
    "      #    y_batch = np.append(y_batch,labeling_subjects(x),axis=0)\n",
    "      #y_batch\n",
    "      #y_batch = y_batch.reshape(len(r_index),len(label_class))\n",
    "      y_batch = to_categorical(y_train_class[batch[1]].values,20)\n",
    "\n",
    "      model.fit(X_batch,y_batch,epochs=1,batch_size=200)\n",
    "  y_pred = model.predict_classes(tfidf_test)\n",
    "  from sklearn.metrics import classification_report,accuracy_score,cohen_kappa_score\n",
    "  acc = accuracy_score(y_test_class,y_pred)\n",
    "  kap = cohen_kappa_score(y_test_class,y_pred)\n",
    "  print(\"accuracy:\",acc)\n",
    "  print(\"kappa:\",kap)\n",
    "  print(classification_report(y_test_class,y_pred))\n",
    "  model.save(\"Dataset/arxiv/nn-model-drop-c{}.h5\".format(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "j1PnaBSnYU-6"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "gz05Te8XYktZ"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QgzWdAwqYqDP"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score,cohen_kappa_score\n",
    "acc = accuracy_score(y_test_class,y_pred)\n",
    "kap = cohen_kappa_score(y_test_class,y_pred)\n",
    "print(\"accuracy:\",acc)\n",
    "print(\"kappa:\",kap)\n",
    "print(classification_report(y_test_class,y_pred))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "third_model_tf_nn strata dropout",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
