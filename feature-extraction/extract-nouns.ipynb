{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_tags = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "def is_noun(pos):\n",
    "    if pos in noun_tags:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(text):\n",
    "    nouns = []\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        sentence_nouns = [word.lower() for (word, pos) in pos_tag(tokens) if is_noun(pos)] \n",
    "        nouns += sentence_nouns\n",
    "    return ' '.join(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories(categories):\n",
    "    specific_categories = categories.split()\n",
    "    top_level_categories = set([category.split('.')[0] for category in specific_categories])\n",
    "    \n",
    "    return specific_categories, list(top_level_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(input_file, output_file):\n",
    "    o = open(output_file, 'w')\n",
    "    with open(input_file, 'r') as f:\n",
    "        for line in f:\n",
    "            input_row = json.loads(line)['arXiv']\n",
    "            \n",
    "            output_row = {}\n",
    "            output_row['categories'], output_row['top_level_categories'] = get_categories(input_row['categories'])\n",
    "\n",
    "            output_row['nouns'] = (extract_nouns(input_row['title']) + ' ' + extract_nouns(input_row['abstract'])).strip()\n",
    "            \n",
    "            json.dump(output_row, o)\n",
    "            o.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set: 2016\n",
    "# Test set: 2017\n",
    "\n",
    "#os.mkdir(os.path.join('..', 'features'))\n",
    "#os.mkdir(os.path.join('..', 'features', 'nouns'))\n",
    "\n",
    "generate_dataset(os.path.join('..', 'data-by-year', '2016.json'), os.path.join('..', 'features', 'nouns', '2016.json'))\n",
    "generate_dataset(os.path.join('..', 'data-by-year', '2017.json'), os.path.join('..', 'features', 'nouns', '2017.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
