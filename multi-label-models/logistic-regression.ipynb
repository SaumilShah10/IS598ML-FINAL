{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function code from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    #    print(\"Normalized confusion matrix\")\n",
    "    #else:\n",
    "    #    print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute accuracy, precision, and recall\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    exact_match = 0\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "\n",
    "    for i in range(len(y_true)):\n",
    "        y = set(y_true[i])\n",
    "        z = set(y_pred[i])\n",
    "\n",
    "        if y == z:\n",
    "            exact_match += 1\n",
    "\n",
    "        accuracy += len(y.intersection(z)) / len(y.union(z))\n",
    "\n",
    "        if len(z) > 0:\n",
    "            precision += len(y.intersection(z)) / len(z)\n",
    "        recall += len(y.intersection(z)) / len(y)\n",
    "\n",
    "    exact_match /= len(y_true)\n",
    "    accuracy /= len(y_true)\n",
    "    precision /= len(y_true)\n",
    "    recall /= len(y_true)\n",
    "\n",
    "    print('Exact match: {0:.2f}'.format(exact_match))\n",
    "    print('Accuracy: {0:.2f}'.format(accuracy))\n",
    "    print('Precision: {0:.2f}'.format(precision))\n",
    "    print('Recall: {0:.2f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training and test data\n",
    "\n",
    "categories = set()\n",
    "top_level_categories = set()\n",
    "\n",
    "train_X = []\n",
    "train_specific_Y = []\n",
    "train_top_Y = []\n",
    "with open(os.path.join('..', 'features', 'nouns', '2016.json'), 'r') as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        \n",
    "        train_X.append(row['nouns'])\n",
    "        train_specific_Y.append(row['categories'])\n",
    "        train_top_Y.append(row['top_level_categories'])\n",
    "        \n",
    "        for category in row['categories']:\n",
    "            categories.add(category)\n",
    "        \n",
    "        for category in row['top_level_categories']:\n",
    "            top_level_categories.add(category)\n",
    "\n",
    "test_X = []\n",
    "test_specific_Y = []\n",
    "test_top_Y = []    \n",
    "with open(os.path.join('..', 'features', 'nouns', '2017.json'), 'r') as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line)\n",
    "        \n",
    "        test_X.append(row['nouns'])\n",
    "        test_specific_Y.append(row['categories'])\n",
    "        test_top_Y.append(row['top_level_categories'])\n",
    "        \n",
    "        for category in row['categories']:\n",
    "            categories.add(category)\n",
    "        \n",
    "        for category in row['top_level_categories']:\n",
    "            top_level_categories.add(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sorted list of categories\n",
    "top_level_categories = list(top_level_categories)\n",
    "top_level_categories.sort()\n",
    "\n",
    "specific_categories = list(categories)\n",
    "specific_categories.sort()\n",
    "\n",
    "# binarize training and test labels\n",
    "mlb_top = MultiLabelBinarizer(top_level_categories)\n",
    "bin_train_top_Y = mlb_top.fit_transform(train_top_Y)\n",
    "bin_test_top_Y = mlb_top.fit_transform(test_top_Y)\n",
    "\n",
    "mlb_specific = MultiLabelBinarizer(specific_categories)\n",
    "bin_train_specific_Y = mlb_specific.fit_transform(train_specific_Y)\n",
    "bin_test_specific_Y = mlb_specific.fit_transform(test_specific_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTRO-PH\n",
      "Instances: 15127\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.94\n",
      "Recall is 0.86\n",
      "\n",
      "COND-MAT\n",
      "Instances: 17576\n",
      "Test accuracy is 0.95\n",
      "Precision is 0.88\n",
      "Recall is 0.78\n",
      "\n",
      "CS\n",
      "Instances: 30686\n",
      "Test accuracy is 0.94\n",
      "Precision is 0.90\n",
      "Recall is 0.85\n",
      "\n",
      "ECON\n",
      "Instances: 109\n",
      "Test accuracy is 1.00\n",
      "Precision is 0.00\n",
      "Recall is 0.00\n",
      "\n",
      "EESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ischool-user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances: 698\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.00\n",
      "Recall is 0.00\n",
      "\n",
      "GR-QC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ischool-user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances: 4589\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.82\n",
      "Recall is 0.57\n",
      "\n",
      "HEP-EX\n",
      "Instances: 2514\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.70\n",
      "Recall is 0.46\n",
      "\n",
      "HEP-LAT\n",
      "Instances: 1044\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.86\n",
      "Recall is 0.40\n",
      "\n",
      "HEP-PH\n",
      "Instances: 6539\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.83\n",
      "Recall is 0.67\n",
      "\n",
      "HEP-TH\n",
      "Instances: 6209\n",
      "Test accuracy is 0.97\n",
      "Precision is 0.81\n",
      "Recall is 0.55\n",
      "\n",
      "MATH\n",
      "Instances: 38456\n",
      "Test accuracy is 0.92\n",
      "Precision is 0.89\n",
      "Recall is 0.86\n",
      "\n",
      "MATH-PH\n",
      "Instances: 3765\n",
      "Test accuracy is 0.97\n",
      "Precision is 0.60\n",
      "Recall is 0.10\n",
      "\n",
      "NLIN\n",
      "Instances: 1834\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.80\n",
      "Recall is 0.18\n",
      "\n",
      "NUCL-EX\n",
      "Instances: 1216\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.67\n",
      "Recall is 0.30\n",
      "\n",
      "NUCL-TH\n",
      "Instances: 2301\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.76\n",
      "Recall is 0.42\n",
      "\n",
      "PHYSICS\n",
      "Instances: 14548\n",
      "Test accuracy is 0.92\n",
      "Precision is 0.77\n",
      "Recall is 0.46\n",
      "\n",
      "Q-BIO\n",
      "Instances: 2482\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.76\n",
      "Recall is 0.37\n",
      "\n",
      "Q-FIN\n",
      "Instances: 895\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.84\n",
      "Recall is 0.37\n",
      "\n",
      "QUANT-PH\n",
      "Instances: 7020\n",
      "Test accuracy is 0.97\n",
      "Precision is 0.84\n",
      "Recall is 0.59\n",
      "\n",
      "STAT\n",
      "Instances: 7538\n",
      "Test accuracy is 0.96\n",
      "Precision is 0.80\n",
      "Recall is 0.46\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Predict top-level categories\n",
    "### One vs rest classifier\n",
    "### Features: TFIDF (top 10000 nouns from the WHOLE training set)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(max_features = 10000)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "predictions_all_tfidf = []\n",
    "for i in range(len(top_level_categories)):\n",
    "    print('{}'.format(mlb_top.classes_[i].upper()))\n",
    "    pipeline.fit(train_X, bin_train_top_Y[: ,i])\n",
    "    \n",
    "    prediction = pipeline.predict(test_X)\n",
    "    predictions_all_tfidf.append(prediction)\n",
    "    \n",
    "    print('Instances: {}'.format(np.sum(bin_test_top_Y[:,i])))\n",
    "    print('Test accuracy is {0:.2f}'.format(accuracy_score(bin_test_top_Y[:,i], prediction)))\n",
    "    print('Precision is {0:.2f}'.format(precision_score(bin_test_top_Y[:,i], prediction)))\n",
    "    print('Recall is {0:.2f}\\n'.format(recall_score(bin_test_top_Y[:,i], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: 0.63\n",
      "Accuracy: 0.74\n",
      "Precision: 0.82\n",
      "Recall: 0.77\n"
     ]
    }
   ],
   "source": [
    "predictions_all_tfidf = mlb_top.inverse_transform(np.transpose(np.asarray(predictions_all_tfidf)))\n",
    "evaluate_predictions(test_top_Y, predictions_all_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTRO-PH\n",
      "Instances: 15127\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.94\n",
      "Recall is 0.85\n",
      "\n",
      "COND-MAT\n",
      "Instances: 17576\n",
      "Test accuracy is 0.95\n",
      "Precision is 0.88\n",
      "Recall is 0.78\n",
      "\n",
      "CS\n",
      "Instances: 30686\n",
      "Test accuracy is 0.94\n",
      "Precision is 0.90\n",
      "Recall is 0.85\n",
      "\n",
      "ECON\n",
      "Instances: 109\n",
      "Test accuracy is 1.00\n",
      "Precision is 0.00\n",
      "Recall is 0.00\n",
      "\n",
      "EESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ischool-user/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances: 698\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.00\n",
      "Recall is 0.00\n",
      "\n",
      "GR-QC\n",
      "Instances: 4589\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.83\n",
      "Recall is 0.52\n",
      "\n",
      "HEP-EX\n",
      "Instances: 2514\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.71\n",
      "Recall is 0.43\n",
      "\n",
      "HEP-LAT\n",
      "Instances: 1044\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.91\n",
      "Recall is 0.30\n",
      "\n",
      "HEP-PH\n",
      "Instances: 6539\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.85\n",
      "Recall is 0.65\n",
      "\n",
      "HEP-TH\n",
      "Instances: 6209\n",
      "Test accuracy is 0.97\n",
      "Precision is 0.81\n",
      "Recall is 0.52\n",
      "\n",
      "MATH\n",
      "Instances: 38456\n",
      "Test accuracy is 0.92\n",
      "Precision is 0.89\n",
      "Recall is 0.86\n",
      "\n",
      "MATH-PH\n",
      "Instances: 3765\n",
      "Test accuracy is 0.97\n",
      "Precision is 0.59\n",
      "Recall is 0.09\n",
      "\n",
      "NLIN\n",
      "Instances: 1834\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.81\n",
      "Recall is 0.14\n",
      "\n",
      "NUCL-EX\n",
      "Instances: 1216\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.70\n",
      "Recall is 0.25\n",
      "\n",
      "NUCL-TH\n",
      "Instances: 2301\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.77\n",
      "Recall is 0.37\n",
      "\n",
      "PHYSICS\n",
      "Instances: 14548\n",
      "Test accuracy is 0.92\n",
      "Precision is 0.78\n",
      "Recall is 0.45\n",
      "\n",
      "Q-BIO\n",
      "Instances: 2482\n",
      "Test accuracy is 0.98\n",
      "Precision is 0.80\n",
      "Recall is 0.31\n",
      "\n",
      "Q-FIN\n",
      "Instances: 895\n",
      "Test accuracy is 0.99\n",
      "Precision is 0.89\n",
      "Recall is 0.27\n",
      "\n",
      "QUANT-PH\n",
      "Instances: 7020\n",
      "Test accuracy is 0.97\n",
      "Precision is 0.85\n",
      "Recall is 0.56\n",
      "\n",
      "STAT\n",
      "Instances: 7538\n",
      "Test accuracy is 0.96\n",
      "Precision is 0.81\n",
      "Recall is 0.44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Predict top-level categories\n",
    "### One vs rest classifier\n",
    "### Features: TFIDF (top 10000 nouns from the POSITIVE training set)\n",
    "\n",
    "predictions_pos_tfidf = []\n",
    "for i in range(len(top_level_categories)):\n",
    "    print('{}'.format(mlb_top.classes_[i].upper()))\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features = 10000)\n",
    "    tfidf_matrix =  vectorizer.fit_transform(np.array(train_X)[bin_train_top_Y[:,i] == 1])\n",
    "    \n",
    "    clf = LogisticRegression(solver='sag')\n",
    "    clf.fit(vectorizer.transform(train_X), bin_train_top_Y[:,i])\n",
    "    \n",
    "    # compute the testing accuracy\n",
    "    prediction = clf.predict(vectorizer.transform(test_X))\n",
    "    predictions_pos_tfidf.append(prediction)\n",
    "    \n",
    "    print('Instances: {}'.format(np.sum(bin_test_top_Y[:,i])))\n",
    "    print('Test accuracy is {0:.2f}'.format(accuracy_score(bin_test_top_Y[:,i], prediction)))\n",
    "    print('Precision is {0:.2f}'.format(precision_score(bin_test_top_Y[:,i], prediction)))\n",
    "    print('Recall is {0:.2f}\\n'.format(recall_score(bin_test_top_Y[:,i], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: 0.63\n",
      "Accuracy: 0.73\n",
      "Precision: 0.81\n",
      "Recall: 0.76\n"
     ]
    }
   ],
   "source": [
    "predictions_pos_tfidf = mlb_top.inverse_transform(np.transpose(np.asarray(predictions_pos_tfidf)))\n",
    "evaluate_predictions(test_top_Y, predictions_pos_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Predict specific categories\n",
    "# ### One vs rest classifier\n",
    "# ### Features: TFIDF (top 10000 nouns from the WHOLE training set)\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#                 ('tfidf', TfidfVectorizer(max_features = 10000)),\n",
    "#                 ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "#             ])\n",
    "\n",
    "# predictions_all_tfidf = []\n",
    "# for i in range(len(specific_categories)):\n",
    "#     print('{}'.format(mlb_specific.classes_[i].upper()))\n",
    "#     pipeline.fit(train_X, bin_train_specific_Y[: ,i])\n",
    "    \n",
    "#     prediction = pipeline.predict(test_X)\n",
    "#     predictions_all_tfidf.append(prediction)\n",
    "    \n",
    "#     print('Instances: {}'.format(np.sum(bin_test_specific_Y[:,i])))\n",
    "#     print('Test accuracy is {0:.2f}'.format(accuracy_score(bin_test_specific_Y[:,i], prediction)))\n",
    "#     print('Precision is {0:.2f}'.format(precision_score(bin_test_specific_Y[:,i], prediction)))\n",
    "#     print('Recall is {0:.2f}\\n'.format(recall_score(bin_test_specific_Y[:,i], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_all_tfidf = mlb_specific.inverse_transform(np.transpose(np.asarray(predictions_all_tfidf)))\n",
    "# evaluate_predictions(test_top_Y, predictions_all_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Predict specific categories\n",
    "# ### One vs rest classifier\n",
    "# ### Features: TFIDF (top 10000 nouns from the POSITIVE training set)\n",
    "\n",
    "# predictions_pos_tfidf = []\n",
    "# for i in range(len(specific_categories)):\n",
    "#     print('{}'.format(mlb_specific.classes_[i].upper()))\n",
    "\n",
    "#     vectorizer = TfidfVectorizer(max_features = 10000)\n",
    "#     tfidf_matrix =  vectorizer.fit_transform(np.array(train_X)[bin_train_specific_Y[:,i] == 1])\n",
    "    \n",
    "#     clf = LogisticRegression(solver='sag')\n",
    "#     clf.fit(vectorizer.transform(train_X), bin_train_specific_Y[:,i])\n",
    "    \n",
    "#     # compute the testing accuracy\n",
    "#     prediction = clf.predict(vectorizer.transform(test_X))\n",
    "#     predictions_pos_tfidf.append(prediction)\n",
    "    \n",
    "#     print('Instances: {}'.format(np.sum(bin_test_specific_Y[:,i])))\n",
    "#     print('Test accuracy is {0:.2f}'.format(accuracy_score(bin_test_specific_Y[:,i], prediction)))\n",
    "#     print('Precision is {0:.2f}'.format(precision_score(bin_test_specific_Y[:,i], prediction)))\n",
    "#     print('Recall is {0:.2f}\\n'.format(recall_score(bin_test_specific_Y[:,i], prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_pos_tfidf = mlb_specific.inverse_transform(np.transpose(np.asarray(predictions_pos_tfidf)))\n",
    "# evaluate_predictions(test_specific_Y, predictions_pos_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true_predictions = np.sum(bin_test_top_Y + transpose_predictions_all_tfidf == 2)\n",
    "# all_predictions = np.count_nonzero(transpose_predictions_all_tfidf)\n",
    "# total_labels = np.count_nonzero(bin_test_top_Y)\n",
    "\n",
    "# print('Exact match: {0:.2f}'.format((len(bin_test_top_Y) - np.count_nonzero(np.sum(bin_test_top_Y != transpose_predictions_all_tfidf, axis = 1))) / len(bin_test_top_Y)))\n",
    "# print('Precision: {0:.2f}'.format(true_predictions/total_labels))\n",
    "# print('Recall: {0:.2f}'.format(true_predictions/all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnf_matrix = confusion_matrix(list(bin_test_top_Y.argmax(axis=1)), list(transpose_predictions_all_tfidf.argmax(axis=1)))\n",
    "\n",
    "# plt.figure(figsize=(16,12))\n",
    "# plot_confusion_matrix(cnf_matrix, classes=top_level_categories, normalize=True, title='Normalized confusion matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
